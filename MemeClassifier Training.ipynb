{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20ad035e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c2b38358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ad4015a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'jpeg', 'png', 'jpg', 'bmp', 'jpe'}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv('./memotion_dataset_7k/labels.csv')\n",
    "df = df.iloc[:,1:]\n",
    "df = df.fillna('')\n",
    "image_names = df['image_name'].tolist()\n",
    "extensions =[]\n",
    "for i in image_names:\n",
    "    extensions.append(i.split('.')[-1].lower())\n",
    "print(set(extensions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "49574585",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#replacing Nan values\n",
    "text = df['text_corrected'].to_numpy()\n",
    "# average = \" \"\n",
    "# text[np.isnan(text)] = average\n",
    "humour = df['humour'].to_numpy()\n",
    "# a,b = np.unique(humour,return_counts=True)\n",
    "# average = a[np.argmax(b)]\n",
    "# humour[np.isnan(humour)]=average\n",
    "\n",
    "sarcasm = df['sarcasm'].to_numpy()\n",
    "# a,b = np.unique(sarcasm,return_counts=True)\n",
    "# average = a[np.argmax(b)]\n",
    "# sarcasm[np.isnan(sarcasm)] = average\n",
    "\n",
    "offensive = df['offensive'].to_numpy()\n",
    "# a,b = np.unique(offensive,return_counts=True)\n",
    "# average = a[np.argmax(b)]\n",
    "# offensive[np.isnan(offensive)] = average\n",
    "\n",
    "motivational = df['motivational'].to_numpy()\n",
    "# a,b = np.unique(motivational,return_counts=True)\n",
    "# average = a[np.argmax(b)]\n",
    "# motivational[np.isnan(motivational)] = average\n",
    "\n",
    "overall_sentiment = df['overall_sentiment'].to_numpy()\n",
    "# a,b = np.unique(overall_sentiment,return_counts=True)\n",
    "# average = a[np.argmax(b)]\n",
    "# overall_sentiment = [average if np.isnan(x) else x for x in overall_sentiment]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dd3dea45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'funny', 'hilarious', 'not_funny', 'very_funny'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(humour)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22c4e8c",
   "metadata": {},
   "source": [
    "# Applying TF_IDF vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "49e62ba1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12657"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#text = 'LOOK THERE MY FRIEND LIGHTYEAR NOW ALL SOHALIKUT TREND PLAY THE 10 YEARS CHALLENGE AT FACEBOOK imgflip.com'\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "text_tfidf = tfidf.fit_transform(text)\n",
    "\n",
    "# Convert the tf-idf matrix to a PyTorch tensor\n",
    "text_tensor = torch.Tensor(text_tfidf.toarray())\n",
    "\n",
    "len(text_tensor)\n",
    "len(text_tensor[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9b9df18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "humour = le.fit_transform(humour)\n",
    "sarcasm = le.fit_transform(sarcasm)\n",
    "offensive = le.fit_transform(offensive) \n",
    "motivational = le.fit_transform(motivational)\n",
    "overall_sentiment = le.fit_transform(overall_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "263828e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3.])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([1,2,3])\n",
    "a.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cb51fc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, image_dir, text_list,humour, sarcasm,offensive,motivational,overall_sentiment, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.text_list = text_list\n",
    "        self.humour = humour\n",
    "        self.sarcasm = sarcasm\n",
    "        self.offensive = offensive\n",
    "        self.motivational = motivational\n",
    "        self.overall_sentiment = overall_sentiment\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # load image and convert to RGB\n",
    "        try:\n",
    "            img_path = os.path.join(self.image_dir, \"image_\"+str(idx)+\".jpg\")\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        except:\n",
    "            try:\n",
    "                img_path = os.path.join(self.image_dir, \"image_\"+str(idx)+\".png\")\n",
    "                image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "\n",
    "            except:\n",
    "                try:\n",
    "                    img_path = os.path.join(self.image_dir, \"image_\"+str(idx)+\".jpeg\")\n",
    "                    image = Image.open(img_path).convert('RGB')\n",
    "                except:\n",
    "                    try:\n",
    "                        img_path = os.path.join(self.image_dir, \"image_\"+str(idx)+\".bmp\")\n",
    "                        image = Image.open(img_path).convert('RGB')\n",
    "                    except:\n",
    "                        img_path = os.path.join(self.image_dir, \"image_\"+str(idx)+\".jpe\")\n",
    "                        image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "\n",
    "        #transform = transforms.Compose([transforms.PILToTensor()])\n",
    "        transform = transforms.Compose([transforms.Resize((100,100)), transforms.ToTensor()])\n",
    "\n",
    "\n",
    "        # transform = transforms.PILToTensor()\n",
    "        # Convert the PIL image to Torch tensor\n",
    "        \n",
    "        image = transform(image)\n",
    "        image = torch.tensor(image)\n",
    "        image = torch.flatten(image)\n",
    "\n",
    "        text = self.text_list[idx]\n",
    "        humour = self.humour[idx]\n",
    "        sarcasm = self.sarcasm[idx]\n",
    "        offensive = self.offensive[idx]\n",
    "        motivational = self.motivational[idx]\n",
    "        overall_sentiment = self.overall_sentiment[idx]\n",
    "\n",
    "        # return the sample as a dictionary\n",
    "        sample = {'image': image, 'text': text,'humour':humour, 'sarcasm': sarcasm, 'offensive':offensive, 'motivational':motivational , 'overall_sentiment':overall_sentiment}\n",
    "        return sample\n",
    "\n",
    "# Example usage\n",
    "#image_dir = \"./memotion_dataset_7k/images\"\n",
    "image_dir = 'C:\\\\Users\\\\A1D\\\\Desktop\\\\MEME CLASSIFIER (MULTIMODAL)\\\\memotion_dataset_7k\\\\images'\n",
    "\n",
    "dataset = MyDataset(image_dir, text_tensor, humour,sarcasm,offensive,motivational,overall_sentiment, transform=None)\n",
    "dataloader = DataLoader(dataset, batch_size=5, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8516a8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MemeClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MemeClassifier, self).__init__()\n",
    "        #self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n",
    "        #self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        #self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        \n",
    "#         self.conv1 = nn.Conv1d(3000,5,kernel_size=3, padding=1)\n",
    "#         self.conv2 = nn.Conv1d(32, 64, 3, padding=1)\n",
    "#         self.conv3 = nn.Conv1d(64, 128, 3, padding=1)\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(5, 30000, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(30000, 128, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv1d(128, 5, kernel_size=3, padding=1)         \n",
    "        \n",
    "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(128 * 16 * 16, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "\n",
    "        #self.text_emb = nn.Embedding(num_embeddings=10000, embedding_dim=256)\n",
    "        self.text_conv1 = nn.Conv1d(5, 12657, kernel_size=3, padding=1)\n",
    "        self.text_conv2 = nn.Conv1d(12657, 1600, kernel_size=3, padding=1)\n",
    "        self.text_pool = nn.MaxPool1d(2, stride=2)\n",
    "        self.text_fc1 = nn.Linear(1600, 256)\n",
    "        self.text_fc2 = nn.Linear(256, 5)\n",
    "\n",
    "        self.fc3 = nn.Linear(6914, 20)\n",
    "        self.fc4 = nn.Linear(20, 5)\n",
    "\n",
    "    def forward(self, image, text):\n",
    "        x1 = self.conv1(image)\n",
    "        x1 = F.relu(x1)\n",
    "        x1 = self.pool(x1)\n",
    "\n",
    "        x1 = self.conv2(x1)\n",
    "        x1 = F.relu(x1)\n",
    "        x1 = self.pool(x1)\n",
    "\n",
    "        x1 = self.conv3(x1)\n",
    "        x1 = F.relu(x1)\n",
    "        x1 = self.pool(x1)\n",
    "\n",
    "        #x1 = x1.view(-1, 128 * 16 * 16)\n",
    "        #x1 = F.relu(self.fc1(x1))\n",
    "        #x1 = F.relu(self.fc2(x1))\n",
    "\n",
    "        #text = self.text_emb(text)\n",
    "        #text = text.permute(0, 2, 1)\n",
    "\n",
    "        text = self.text_conv1(text)\n",
    "        text = F.relu(text)\n",
    "        text = self.text_pool(text)\n",
    "\n",
    "        text = self.text_conv2(text)\n",
    "        text = F.relu(text)\n",
    "        text = self.text_pool(text)\n",
    "\n",
    "        #text = text.view(-1, 64 * 25)\n",
    "        text = F.relu(self.text_fc1(text.T))\n",
    "        text = F.relu(self.text_fc2(text))\n",
    "        print(\"x1:\",np.shape(x1),\" text: \", np.shape(text.T))\n",
    "        x2 = torch.cat((x1, text.T), dim=1)\n",
    "        x2 = F.relu(self.fc3(x2))\n",
    "        x2 = self.fc4(x2)\n",
    "\n",
    "        output_humor = x2[:, 0]\n",
    "        output_sarcasm = x2[:, 1]\n",
    "        output_offensive = x2[:, 2]\n",
    "        output_motivational = x2[:, 3]\n",
    "        output_overall_sentiment = x2[:,4]\n",
    "        \n",
    "        return output_humor, output_sarcasm, output_offensive, output_motivational, output_overall_sentiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113772c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051ee421",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\A1D\\AppData\\Local\\Temp/ipykernel_14376/3540852770.py:50: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  image = torch.tensor(image)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch of 50 images being processed...s\n",
      "x1: torch.Size([5, 3750])  text:  torch.Size([5, 3164])\n",
      "Batch discarded\n",
      "batch of 50 images being processed...s\n",
      "x1: torch.Size([5, 3750])  text:  torch.Size([5, 3164])\n",
      "Batch discarded\n",
      "batch of 50 images being processed...s\n",
      "x1: torch.Size([5, 3750])  text:  torch.Size([5, 3164])\n",
      "Batch discarded\n",
      "batch of 50 images being processed...s\n",
      "x1: torch.Size([5, 3750])  text:  torch.Size([5, 3164])\n",
      "Batch discarded\n",
      "batch of 50 images being processed...s\n",
      "x1: torch.Size([5, 3750])  text:  torch.Size([5, 3164])\n",
      "Batch discarded\n",
      "batch of 50 images being processed...s\n",
      "x1: torch.Size([5, 3750])  text:  torch.Size([5, 3164])\n",
      "Batch discarded\n",
      "batch of 50 images being processed...s\n",
      "x1: torch.Size([5, 3750])  text:  torch.Size([5, 3164])\n",
      "Batch discarded\n",
      "batch of 50 images being processed...s\n",
      "x1: torch.Size([5, 3750])  text:  torch.Size([5, 3164])\n",
      "Batch discarded\n",
      "batch of 50 images being processed...s\n",
      "x1: torch.Size([5, 3750])  text:  torch.Size([5, 3164])\n",
      "Batch discarded\n",
      "batch of 50 images being processed...s\n",
      "x1: torch.Size([5, 3750])  text:  torch.Size([5, 3164])\n",
      "Batch discarded\n",
      "[Epoch: 1, Batch:    10] Loss: 100.162\n",
      "batch of 50 images being processed...s\n",
      "x1: torch.Size([5, 3750])  text:  torch.Size([5, 3164])\n",
      "Batch discarded\n",
      "batch of 50 images being processed...s\n",
      "x1: torch.Size([5, 3750])  text:  torch.Size([5, 3164])\n",
      "Batch discarded\n",
      "batch of 50 images being processed...s\n",
      "x1: torch.Size([5, 3750])  text:  torch.Size([5, 3164])\n",
      "Batch discarded\n",
      "batch of 50 images being processed...s\n",
      "x1: torch.Size([5, 3750])  text:  torch.Size([5, 3164])\n",
      "Batch discarded\n",
      "batch of 50 images being processed...s\n",
      "x1: torch.Size([5, 3750])  text:  torch.Size([5, 3164])\n",
      "Batch discarded\n",
      "batch of 50 images being processed...s\n",
      "x1: torch.Size([5, 3750])  text:  torch.Size([5, 3164])\n",
      "Batch discarded\n",
      "batch of 50 images being processed...s\n",
      "x1: torch.Size([5, 3750])  text:  torch.Size([5, 3164])\n",
      "Batch discarded\n",
      "batch of 50 images being processed...s\n",
      "x1: torch.Size([5, 3750])  text:  torch.Size([5, 3164])\n",
      "Batch discarded\n",
      "batch of 50 images being processed...s\n",
      "x1: torch.Size([5, 3750])  text:  torch.Size([5, 3164])\n",
      "Batch discarded\n",
      "batch of 50 images being processed...s\n",
      "x1: torch.Size([5, 3750])  text:  torch.Size([5, 3164])\n",
      "Batch discarded\n",
      "[Epoch: 1, Batch:    20] Loss: 38.784\n",
      "batch of 50 images being processed...s\n",
      "x1: torch.Size([5, 3750])  text:  torch.Size([5, 3164])\n",
      "Batch discarded\n",
      "batch of 50 images being processed...s\n",
      "x1: torch.Size([5, 3750])  text:  torch.Size([5, 3164])\n",
      "Batch discarded\n",
      "batch of 50 images being processed...s\n",
      "x1: torch.Size([5, 3750])  text:  torch.Size([5, 3164])\n",
      "Batch discarded\n",
      "batch of 50 images being processed...s\n",
      "x1: torch.Size([5, 3750])  text:  torch.Size([5, 3164])\n",
      "Batch discarded\n",
      "batch of 50 images being processed...s\n",
      "x1: torch.Size([5, 3750])  text:  torch.Size([5, 3164])\n",
      "Batch discarded\n",
      "batch of 50 images being processed...s\n",
      "x1: torch.Size([5, 3750])  text:  torch.Size([5, 3164])\n",
      "Batch discarded\n",
      "batch of 50 images being processed...s\n",
      "x1: torch.Size([5, 3750])  text:  torch.Size([5, 3164])\n",
      "Batch discarded\n",
      "batch of 50 images being processed...s\n",
      "x1: torch.Size([5, 3750])  text:  torch.Size([5, 3164])\n",
      "Batch discarded\n",
      "batch of 50 images being processed...s\n",
      "x1: torch.Size([5, 3750])  text:  torch.Size([5, 3164])\n",
      "Batch discarded\n",
      "batch of 50 images being processed...s\n",
      "x1: torch.Size([5, 3750])  text:  torch.Size([5, 3164])\n",
      "Batch discarded\n",
      "[Epoch: 1, Batch:    30] Loss: 34.601\n",
      "batch of 50 images being processed...s\n",
      "x1: torch.Size([5, 3750])  text:  torch.Size([5, 3164])\n",
      "Batch discarded\n",
      "batch of 50 images being processed...s\n",
      "x1: torch.Size([5, 3750])  text:  torch.Size([5, 3164])\n",
      "Batch discarded\n",
      "batch of 50 images being processed...s\n",
      "x1: torch.Size([5, 3750])  text:  torch.Size([5, 3164])\n",
      "Batch discarded\n",
      "batch of 50 images being processed...s\n",
      "x1: torch.Size([5, 3750])  text:  torch.Size([5, 3164])\n",
      "Batch discarded\n",
      "batch of 50 images being processed...s\n",
      "x1: torch.Size([5, 3750])  text:  torch.Size([5, 3164])\n",
      "Batch discarded\n",
      "batch of 50 images being processed...s\n",
      "x1: torch.Size([5, 3750])  text:  torch.Size([5, 3164])\n",
      "Batch discarded\n",
      "batch of 50 images being processed...s\n",
      "x1: torch.Size([5, 3750])  text:  torch.Size([5, 3164])\n",
      "Batch discarded\n",
      "batch of 50 images being processed...s\n",
      "x1: torch.Size([5, 3750])  text:  torch.Size([5, 3164])\n",
      "Batch discarded\n",
      "batch of 50 images being processed...s\n",
      "x1: torch.Size([5, 3750])  text:  torch.Size([5, 3164])\n",
      "Batch discarded\n",
      "batch of 50 images being processed...s\n",
      "x1: torch.Size([5, 3750])  text:  torch.Size([5, 3164])\n",
      "Batch discarded\n",
      "[Epoch: 1, Batch:    40] Loss: 40.558\n",
      "batch of 50 images being processed...s\n",
      "x1: torch.Size([5, 3750])  text:  torch.Size([5, 3164])\n",
      "Batch discarded\n",
      "batch of 50 images being processed...s\n",
      "x1: torch.Size([5, 3750])  text:  torch.Size([5, 3164])\n",
      "Batch discarded\n",
      "batch of 50 images being processed...s\n",
      "x1: torch.Size([5, 3750])  text:  torch.Size([5, 3164])\n",
      "Batch discarded\n",
      "batch of 50 images being processed...s\n",
      "x1: torch.Size([5, 3750])  text:  torch.Size([5, 3164])\n",
      "Batch discarded\n",
      "batch of 50 images being processed...s\n",
      "x1: torch.Size([5, 3750])  text:  torch.Size([5, 3164])\n",
      "Batch discarded\n",
      "batch of 50 images being processed...s\n",
      "x1: torch.Size([5, 3750])  text:  torch.Size([5, 3164])\n",
      "Batch discarded\n",
      "batch of 50 images being processed...s\n",
      "x1: torch.Size([5, 3750])  text:  torch.Size([5, 3164])\n",
      "Batch discarded\n",
      "batch of 50 images being processed...s\n",
      "x1: torch.Size([5, 3750])  text:  torch.Size([5, 3164])\n",
      "Batch discarded\n",
      "batch of 50 images being processed...s\n",
      "x1: torch.Size([5, 3750])  text:  torch.Size([5, 3164])\n",
      "Batch discarded\n",
      "batch of 50 images being processed...s\n",
      "x1: torch.Size([5, 3750])  text:  torch.Size([5, 3164])\n",
      "Batch discarded\n",
      "[Epoch: 1, Batch:    50] Loss: 37.823\n",
      "batch of 50 images being processed...s\n",
      "x1: torch.Size([5, 3750])  text:  torch.Size([5, 3164])\n",
      "Batch discarded\n",
      "batch of 50 images being processed...s\n",
      "x1: torch.Size([5, 3750])  text:  torch.Size([5, 3164])\n",
      "Batch discarded\n",
      "batch of 50 images being processed...s\n",
      "x1: torch.Size([5, 3750])  text:  torch.Size([5, 3164])\n",
      "Batch discarded\n",
      "batch of 50 images being processed...s\n",
      "x1: torch.Size([5, 3750])  text:  torch.Size([5, 3164])\n",
      "Batch discarded\n",
      "batch of 50 images being processed...s\n",
      "x1: torch.Size([5, 3750])  text:  torch.Size([5, 3164])\n",
      "Batch discarded\n",
      "batch of 50 images being processed...s\n",
      "x1: torch.Size([5, 3750])  text:  torch.Size([5, 3164])\n",
      "Batch discarded\n",
      "batch of 50 images being processed...s\n",
      "x1: torch.Size([5, 3750])  text:  torch.Size([5, 3164])\n",
      "Batch discarded\n",
      "batch of 50 images being processed...s\n",
      "x1: torch.Size([5, 3750])  text:  torch.Size([5, 3164])\n",
      "Batch discarded\n",
      "batch of 50 images being processed...s\n",
      "x1: torch.Size([5, 3750])  text:  torch.Size([5, 3164])\n",
      "Batch discarded\n",
      "batch of 50 images being processed...s\n",
      "x1: torch.Size([5, 3750])  text:  torch.Size([5, 3164])\n",
      "Batch discarded\n",
      "[Epoch: 1, Batch:    60] Loss: 38.465\n",
      "batch of 50 images being processed...s\n",
      "x1: torch.Size([5, 3750])  text:  torch.Size([5, 3164])\n",
      "Batch discarded\n",
      "batch of 50 images being processed...s\n",
      "x1: torch.Size([5, 3750])  text:  torch.Size([5, 3164])\n",
      "Batch discarded\n",
      "batch of 50 images being processed...s\n",
      "x1: torch.Size([5, 3750])  text:  torch.Size([5, 3164])\n",
      "Batch discarded\n",
      "batch of 50 images being processed...s\n",
      "x1: torch.Size([5, 3750])  text:  torch.Size([5, 3164])\n",
      "Batch discarded\n",
      "batch of 50 images being processed...s\n",
      "x1: torch.Size([5, 3750])  text:  torch.Size([5, 3164])\n",
      "Batch discarded\n",
      "batch of 50 images being processed...s\n",
      "x1: torch.Size([5, 3750])  text:  torch.Size([5, 3164])\n",
      "Batch discarded\n",
      "batch of 50 images being processed...s\n",
      "x1: torch.Size([5, 3750])  text:  torch.Size([5, 3164])\n",
      "Batch discarded\n",
      "batch of 50 images being processed...s\n",
      "x1: torch.Size([5, 3750])  text:  torch.Size([5, 3164])\n",
      "Batch discarded\n",
      "batch of 50 images being processed...s\n",
      "x1: torch.Size([5, 3750])  text:  torch.Size([5, 3164])\n",
      "Batch discarded\n",
      "batch of 50 images being processed...s\n",
      "x1: torch.Size([5, 3750])  text:  torch.Size([5, 3164])\n",
      "Batch discarded\n",
      "[Epoch: 1, Batch:    70] Loss: 37.342\n",
      "batch of 50 images being processed...s\n",
      "x1: torch.Size([5, 3750])  text:  torch.Size([5, 3164])\n",
      "Batch discarded\n",
      "batch of 50 images being processed...s\n",
      "x1: torch.Size([5, 3750])  text:  torch.Size([5, 3164])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch discarded\n",
      "batch of 50 images being processed...s\n",
      "x1: torch.Size([5, 3750])  text:  torch.Size([5, 3164])\n",
      "Batch discarded\n",
      "batch of 50 images being processed...s\n",
      "x1: torch.Size([5, 3750])  text:  torch.Size([5, 3164])\n",
      "Batch discarded\n",
      "batch of 50 images being processed...s\n",
      "x1: torch.Size([5, 3750])  text:  torch.Size([5, 3164])\n",
      "Batch discarded\n",
      "batch of 50 images being processed...s\n",
      "x1: torch.Size([5, 3750])  text:  torch.Size([5, 3164])\n",
      "Batch discarded\n",
      "batch of 50 images being processed...s\n",
      "x1: torch.Size([5, 3750])  text:  torch.Size([5, 3164])\n",
      "Batch discarded\n",
      "batch of 50 images being processed...s\n",
      "x1: torch.Size([5, 3750])  text:  torch.Size([5, 3164])\n",
      "Batch discarded\n",
      "batch of 50 images being processed...s\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "model = MemeClassifier()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "num_epochs = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "        inputs, humour, sarcasm, offensive, motivational, overall_sentiment = data['image'],data['humour'], data['sarcasm'], data['offensive'], data['motivational'], data['overall_sentiment']\n",
    "        print(\"batch of 50 images being processed...s\")\n",
    "        humour = humour.float()\n",
    "        sarcasm = sarcasm.float()\n",
    "        offensive = offensive.float()\n",
    "        motivational = motivational.float()\n",
    "        overall_sentiment = overall_sentiment.float()\n",
    "\n",
    "\n",
    "        text = data['text']\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        output_humor, output_sarcasm, output_offensive, output_motivational, output_overall_sentiment = model(inputs, text)\n",
    "        loss_humor = criterion(output_humor, humour)\n",
    "        loss_sarcasm = criterion(output_sarcasm, sarcasm)\n",
    "        loss_offensive = criterion(output_offensive, offensive)\n",
    "        loss_motivational = criterion(output_motivational, motivational)\n",
    "        loss_overall_sentiment = criterion(output_overall_sentiment, overall_sentiment)\n",
    "        loss = loss_humor + loss_sarcasm + loss_offensive + loss_motivational\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print statistics\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        print(\"Batch discarded\")\n",
    "        \n",
    "        if i % 10 == 9:    # Print every 10 mini-batches\n",
    "            print('[Epoch: %d, Batch: %5d] Loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 10))\n",
    "            running_loss = 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7106dd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('meme_model.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "\n",
    "    f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3de0f69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b96db0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7712b13b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b2ee1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
